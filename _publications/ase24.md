---
title: "LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation"
collection: publications
category: conferences
permalink: /publication/ase24unittest
date: 2024-10-27
venue: 'ASE 24: Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering'
paperurl: 'https://dl.acm.org/doi/abs/10.1145/3691620.3695330'
citation: 'Ouedraogo, Wendkuuni C., Kader Kabore, Haoye Tian, Yewei Song, Anil Koyuncu, Jacques Klein, David Lo, and Tegawende F. Bissyande. "Llms and prompting for unit test generation: A large-scale evaluation." In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, pp. 2464-2465. 2024.'
---

Unit testing, essential for identifying bugs, is often neglected due to time constraints. Automated test generation tools exist but typically lack readability and require developer intervention. Large Language Models (LLMs) like GPT and Mistral show potential in test generation, but their effectiveness remains unclear.
This study evaluates four LLMs and five prompt engineering techniques, analyzing 216 300 tests for 690 Java classes from diverse datasets. We assess correctness, readability, coverage, and bug detection, comparing LLM-generated tests to EvoSuite. While LLMs show promise, improvements in correctness are needed. The study highlights both the strengths and limitations of LLMs, offering insights for future research.